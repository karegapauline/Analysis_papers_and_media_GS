{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNq43JPkkTTDyrvvEia/1sM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karegapauline/Analysis_papers_and_media_GS/blob/main/metaanalysis_mediaarticles_alone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Methodology Part 2\n",
        "\n",
        "Article search to gauge how the media portrays air quality and health We use Feedparser v6.0.11 to scrape news feed data. Feedparser is used to download and parse feeds. We obtained news articles about air quality and health in Kenya, South Africa, and the United Kingdom. We targeted media houses that offered both digital and print articles. For Kenya, we used Nation Africa, Standard Media, and The Star. South Africa, Daily Maverick, timeslive, and news24. And for the UK, BBC, The Guardian, and Telegraph. We did not specify any timelines and gathered all articles. Our search terms were as follows: ”air pollution”, ”air quality”, \"climate change\", ”respiratory diseases”, and ”pollution policy”."
      ],
      "metadata": {
        "id": "LNz2WUTigEZH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PW8ubI42f8p2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6e595d6-77bd-4429-d2a0-af244b78567c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=6199a6337f7e295f3a1ce9726a6da4a3d524d17b5af39b272936cbca186614ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.12 sgmllib3k-1.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install feedparser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "import pandas as pd\n",
        "import time\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "# ------------------------\n",
        "# CONFIGURATION\n",
        "# ------------------------\n",
        "\n",
        "SEARCH_TERMS = [\n",
        "    \"air pollution\",\n",
        "    \"air quality\",\n",
        "    \"respiratory illness\",\n",
        "    \"respiratory disease\",\n",
        "    \"air quality policy\"\n",
        "]\n",
        "\n",
        "COUNTRY_SOURCES = {\n",
        "    \"Kenya\": [\"nation.africa\", \"standardmedia.co.ke\", \"the-star.co.ke\"],\n",
        "    \"South Africa\": [\"dailymaverick.co.za\", \"timeslive.co.za\", \"news24.com\"],\n",
        "    \"UK\": [\"bbc.co.uk\", \"theguardian.com\", \"telegraph.co.uk\"]\n",
        "}\n",
        "\n",
        "# ------------------------\n",
        "# FUNCTION TO PARSE GOOGLE RSS\n",
        "# ------------------------\n",
        "\n",
        "def fetch_articles(search_term, site):\n",
        "    query = f'{search_term} site:{site}'\n",
        "    encoded_query = quote_plus(query)\n",
        "    url = f\"https://news.google.com/rss/search?q={encoded_query}&hl=en-GB&gl=GB&ceid=GB:en\"\n",
        "\n",
        "    feed = feedparser.parse(url)\n",
        "    articles = []\n",
        "\n",
        "    for entry in feed.entries:\n",
        "        articles.append({\n",
        "            \"search_term\": search_term,\n",
        "            \"source_site\": site,\n",
        "            \"title\": entry.title,\n",
        "            \"link\": entry.link,\n",
        "            \"published\": entry.get(\"published\", \"\"),\n",
        "            \"summary\": entry.get(\"summary\", \"\")\n",
        "        })\n",
        "\n",
        "    return articles\n",
        "\n",
        "# ------------------------\n",
        "# MAIN FUNCTION\n",
        "# ------------------------\n",
        "\n",
        "def scrape_google_news():\n",
        "    for country, sources in COUNTRY_SOURCES.items():\n",
        "        print(f\"\\n Scraping Google News for {country}...\")\n",
        "        all_records = []\n",
        "\n",
        "        for site in sources:\n",
        "            for term in SEARCH_TERMS:\n",
        "                print(f\"🔍 {term} @ {site}\")\n",
        "                articles = fetch_articles(term, site)\n",
        "                all_records.extend(articles)\n",
        "                time.sleep(1)  # be polite to Google servers\n",
        "\n",
        "        # Save to CSV\n",
        "        df = pd.DataFrame(all_records)\n",
        "        filename = f\"{country.lower().replace(' ', '_')}_gnews.csv\"\n",
        "        df.to_csv(filename, index=False)\n",
        "        print(f\"✅ Saved {len(df)} articles to {filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    scrape_google_news()\n"
      ],
      "metadata": {
        "id": "dXL5uxTYgJIa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c9e4ad4-6c4e-474a-bd2c-83064456a34f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Scraping Google News for Kenya...\n",
            "🔍 air pollution @ nation.africa\n",
            "🔍 air quality @ nation.africa\n",
            "🔍 respiratory illness @ nation.africa\n",
            "🔍 respiratory disease @ nation.africa\n",
            "🔍 air quality policy @ nation.africa\n",
            "🔍 air pollution @ standardmedia.co.ke\n",
            "🔍 air quality @ standardmedia.co.ke\n",
            "🔍 respiratory illness @ standardmedia.co.ke\n",
            "🔍 respiratory disease @ standardmedia.co.ke\n",
            "🔍 air quality policy @ standardmedia.co.ke\n",
            "🔍 air pollution @ the-star.co.ke\n",
            "🔍 air quality @ the-star.co.ke\n",
            "🔍 respiratory illness @ the-star.co.ke\n",
            "🔍 respiratory disease @ the-star.co.ke\n",
            "🔍 air quality policy @ the-star.co.ke\n",
            "✅ Saved 1479 articles to kenya_gnews.csv\n",
            "\n",
            " Scraping Google News for South Africa...\n",
            "🔍 air pollution @ dailymaverick.co.za\n",
            "🔍 air quality @ dailymaverick.co.za\n",
            "🔍 respiratory illness @ dailymaverick.co.za\n",
            "🔍 respiratory disease @ dailymaverick.co.za\n",
            "🔍 air quality policy @ dailymaverick.co.za\n",
            "🔍 air pollution @ timeslive.co.za\n",
            "🔍 air quality @ timeslive.co.za\n",
            "🔍 respiratory illness @ timeslive.co.za\n",
            "🔍 respiratory disease @ timeslive.co.za\n",
            "🔍 air quality policy @ timeslive.co.za\n",
            "🔍 air pollution @ news24.com\n",
            "🔍 air quality @ news24.com\n",
            "🔍 respiratory illness @ news24.com\n",
            "🔍 respiratory disease @ news24.com\n",
            "🔍 air quality policy @ news24.com\n",
            "✅ Saved 1500 articles to south_africa_gnews.csv\n",
            "\n",
            " Scraping Google News for UK...\n",
            "🔍 air pollution @ bbc.co.uk\n",
            "🔍 air quality @ bbc.co.uk\n",
            "🔍 respiratory illness @ bbc.co.uk\n",
            "🔍 respiratory disease @ bbc.co.uk\n",
            "🔍 air quality policy @ bbc.co.uk\n",
            "🔍 air pollution @ theguardian.com\n",
            "🔍 air quality @ theguardian.com\n",
            "🔍 respiratory illness @ theguardian.com\n",
            "🔍 respiratory disease @ theguardian.com\n",
            "🔍 air quality policy @ theguardian.com\n",
            "🔍 air pollution @ telegraph.co.uk\n",
            "🔍 air quality @ telegraph.co.uk\n",
            "🔍 respiratory illness @ telegraph.co.uk\n",
            "🔍 respiratory disease @ telegraph.co.uk\n",
            "🔍 air quality policy @ telegraph.co.uk\n",
            "✅ Saved 1500 articles to uk_gnews.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## FITERING OF RELEVANT ARTICLES\n",
        "# REMOVE DUPLICATES first\n",
        "import pandas as pd\n",
        "\n",
        "# Load your file\n",
        "df = pd.read_csv(\"kenya_gnews.csv\")\n",
        "df2 = pd.read_csv(\"uk_gnews.csv\")\n",
        "df3 = pd.read_csv(\"south_africa_gnews.csv\")\n",
        "\n",
        "# Normalize titles\n",
        "df['clean_title'] = df['title'].str.lower().str.strip()\n",
        "df2['clean_title'] = df2['title'].str.lower().str.strip()\n",
        "df3['clean_title'] = df3['title'].str.lower().str.strip()\n",
        "\n",
        "# Mark duplicates\n",
        "df['duplicate'] = df.duplicated(subset='clean_title', keep='first')\n",
        "df2['duplicate'] = df2.duplicated(subset='clean_title', keep='first')\n",
        "df3['duplicate'] = df3.duplicated(subset='clean_title', keep='first')\n",
        "\n",
        "# Save with duplicate flag\n",
        "df.to_csv(\"kenya_gnews_deduped.csv\", index=False)\n",
        "\n",
        "df2.to_csv(\"uk_gnews_deduped.csv\", index=False)\n",
        "\n",
        "df3.to_csv(\"south_africa_gnews_deduped.csv\", index=False)\n",
        "\n",
        "# Save only unique articles\n",
        "df[~df['duplicate']].to_csv(\"kenya_gnews_unique.csv\", index=False)\n",
        "\n",
        "df2[~df2['duplicate']].to_csv(\"uk_gnews_unique.csv\", index=False)\n",
        "\n",
        "df3[~df3['duplicate']].to_csv(\"south_africa_gnews_unique.csv\", index=False)\n",
        "\n",
        "print(f\"✅ Found and removed {df['duplicate'].sum()} duplicates.\")\n",
        "print(f\"✅ Found and removed {df2['duplicate'].sum()} duplicates.\")\n",
        "print(f\"✅ Found and removed {df3['duplicate'].sum()} duplicates.\")"
      ],
      "metadata": {
        "id": "RFqjBw_PgWKY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a26c1fdf-4575-498f-cce4-5ab298b83ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Found and removed 638 duplicates.\n",
            "✅ Found and removed 565 duplicates.\n",
            "✅ Found and removed 562 duplicates.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## NOW REMOVE ALL HEADINGS THAT ARE NOT AIR QUALITY AND HEALTH RELATED\n",
        "import pandas as pd\n",
        "\n",
        "# Load your file\n",
        "df = pd.read_csv(\"kenya_gnews_unique.csv\")\n",
        "df2 = pd.read_csv(\"uk_gnews_unique.csv\")\n",
        "df3 = pd.read_csv(\"south_africa_gnews_unique.csv\")\n",
        "\n",
        "# Filter titles that mention \"air quality\" or \"health\"\n",
        "df_filtered = df[df['title'].str.contains(r'\\b(air quality| pollution | cooking | electric | climate | diseases | breathing | cities | environmental |health)\\b', case=False, na=False)]\n",
        "df2_filtered = df2[df2['title'].str.contains(r'\\b(air quality| pollution | cooking | electric | climate | diseases | breathing | cities | environmental |health)\\b', case=False, na=False)]\n",
        "df3_filtered = df3[df3['title'].str.contains(r'\\b(air quality| pollution | cooking | electric | climate | diseases | breathing | cities | environmental |health)\\b', case=False, na=False)]\n",
        "\n",
        "\n",
        "\n",
        "# Normalize titles for deduplication\n",
        "df_filtered['clean_title'] = df_filtered['title'].str.lower().str.strip()\n",
        "df2_filtered['clean_title'] = df2_filtered['title'].str.lower().str.strip()\n",
        "df3_filtered['clean_title'] = df3_filtered['title'].str.lower().str.strip()\n",
        "\n",
        "# Mark exact duplicates\n",
        "df_filtered['duplicate'] = df_filtered.duplicated(subset='clean_title', keep='first')\n",
        "df2_filtered['duplicate'] = df2_filtered.duplicated(subset='clean_title', keep='first')\n",
        "df3_filtered['duplicate'] = df3_filtered.duplicated(subset='clean_title', keep='first')\n",
        "\n",
        "# Save filtered and deduplicated articles\n",
        "df_filtered.to_csv(\"kenya_gnews_filtered_deduped.csv\", index=False)\n",
        "df2_filtered.to_csv(\"uk_gnews_filtered_deduped.csv\", index=False)\n",
        "df3_filtered.to_csv(\"south_africa_gnews_filtered_deduped.csv\", index=False)\n",
        "\n",
        "# Save only unique ones\n",
        "df_filtered[~df_filtered['duplicate']].to_csv(\"kenya_gnews_filtered_unique.csv\", index=False)\n",
        "df2_filtered[~df2_filtered['duplicate']].to_csv(\"uk_gnews_filtered_unique.csv\", index=False)\n",
        "df3_filtered[~df3_filtered['duplicate']].to_csv(\"south_africa_gnews_filtered_unique.csv\", index=False)\n",
        "\n",
        "print(f\"✅ Filtered to {len(df_filtered)} relevant articles, removed {df_filtered['duplicate'].sum()} duplicates.\")\n",
        "print(f\"✅ Filtered to {len(df2_filtered)} relevant articles, removed {df2_filtered['duplicate'].sum()} duplicates.\")\n",
        "print(f\"✅ Filtered to {len(df3_filtered)} relevant articles, removed {df3_filtered['duplicate'].sum()} duplicates.\")\n"
      ],
      "metadata": {
        "id": "guKS7mYMgaq2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b69ce26a-b9ac-4899-975e-68e1500b2e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Filtered to 147 relevant articles, removed 0 duplicates.\n",
            "✅ Filtered to 139 relevant articles, removed 0 duplicates.\n",
            "✅ Filtered to 134 relevant articles, removed 0 duplicates.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-535183178.py:10: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
            "  df_filtered = df[df['title'].str.contains(r'\\b(air quality| pollution | cooking | electric | climate | diseases | breathing | cities | environmental |health)\\b', case=False, na=False)]\n",
            "/tmp/ipython-input-535183178.py:11: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
            "  df2_filtered = df2[df2['title'].str.contains(r'\\b(air quality| pollution | cooking | electric | climate | diseases | breathing | cities | environmental |health)\\b', case=False, na=False)]\n",
            "/tmp/ipython-input-535183178.py:12: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
            "  df3_filtered = df3[df3['title'].str.contains(r'\\b(air quality| pollution | cooking | electric | climate | diseases | breathing | cities | environmental |health)\\b', case=False, na=False)]\n",
            "/tmp/ipython-input-535183178.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_filtered['clean_title'] = df_filtered['title'].str.lower().str.strip()\n",
            "/tmp/ipython-input-535183178.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df2_filtered['clean_title'] = df2_filtered['title'].str.lower().str.strip()\n",
            "/tmp/ipython-input-535183178.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df3_filtered['clean_title'] = df3_filtered['title'].str.lower().str.strip()\n",
            "/tmp/ipython-input-535183178.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_filtered['duplicate'] = df_filtered.duplicated(subset='clean_title', keep='first')\n",
            "/tmp/ipython-input-535183178.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df2_filtered['duplicate'] = df2_filtered.duplicated(subset='clean_title', keep='first')\n",
            "/tmp/ipython-input-535183178.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df3_filtered['duplicate'] = df3_filtered.duplicated(subset='clean_title', keep='first')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final iteration run on October 3rd 2025.\n",
        "\n",
        "147 articles were obtained for Kenya, 139 for the UK, and 134 for SA. this was after deduplication and filtering of articles that weren't releted to air quality and health."
      ],
      "metadata": {
        "id": "GqaMHLGESC7P"
      }
    }
  ]
}